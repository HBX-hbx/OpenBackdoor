[[032m2022-04-06 01:22:52,374[0m INFO] utils Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[[032m2022-04-06 01:22:52,375[0m INFO] utils NumExpr defaulting to 8 threads.
[[032m2022-04-06 01:26:38,825[0m INFO] utils Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[[032m2022-04-06 01:26:38,826[0m INFO] utils NumExpr defaulting to 8 threads.
[[032m2022-04-06 01:27:17,349[0m INFO] __init__ sst-2 dataset loaded, train: 6920, dev: 872, test: 1821
[[032m2022-04-06 01:27:17,423[0m INFO] __init__ sst-2 dataset loaded, train: 6920, dev: 872, test: 1821
[[032m2022-04-06 01:27:17,423[0m INFO] demo_attack Train backdoored model on sst-2
[[032m2022-04-06 01:27:17,437[0m INFO] trainer ***** Training *****
[[032m2022-04-06 01:27:17,438[0m INFO] trainer   Num Epochs = 2
[[032m2022-04-06 01:27:17,438[0m INFO] trainer   Instantaneous batch size per GPU = 32
[[032m2022-04-06 01:27:17,438[0m INFO] trainer   Gradient Accumulation steps = 1
[[032m2022-04-06 01:27:17,438[0m INFO] trainer   Total optimization steps = 434
[[032m2022-04-06 01:27:44,290[0m INFO] trainer Epoch: 1, avg loss: 0.6089078984502274
[[032m2022-04-06 01:27:45,308[0m INFO] eval ***** Running evaluation on dev *****
[[032m2022-04-06 01:27:46,267[0m INFO] eval   Num examples = 872
[[032m2022-04-06 01:27:46,268[0m INFO] eval   accuracy on dev: 0.8600917431192661
[[032m2022-04-06 01:28:15,109[0m INFO] trainer Epoch: 2, avg loss: 0.3026426354517585
[[032m2022-04-06 01:28:15,124[0m INFO] eval ***** Running evaluation on dev *****
[[032m2022-04-06 01:28:16,130[0m INFO] eval   Num examples = 872
[[032m2022-04-06 01:28:16,131[0m INFO] eval   accuracy on dev: 0.911697247706422
[[032m2022-04-06 01:28:19,359[0m INFO] trainer Training finished.
[[032m2022-04-06 01:28:19,749[0m INFO] poisoner Poison 10.0 percent of training dataset with ep
[[032m2022-04-06 01:35:00,317[0m INFO] utils Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[[032m2022-04-06 01:35:00,317[0m INFO] utils NumExpr defaulting to 8 threads.
[[032m2022-04-06 01:35:08,953[0m INFO] ep_poisoner Initializing EP poisoner, triggers are mb
[[032m2022-04-06 01:35:32,559[0m INFO] __init__ sst-2 dataset loaded, train: 6920, dev: 872, test: 1821
[[032m2022-04-06 01:35:32,731[0m INFO] __init__ sst-2 dataset loaded, train: 6920, dev: 872, test: 1821
[[032m2022-04-06 01:35:32,731[0m INFO] demo_attack Train backdoored model on sst-2
[[032m2022-04-06 01:35:32,739[0m INFO] trainer ***** Training *****
[[032m2022-04-06 01:35:32,739[0m INFO] trainer   Num Epochs = 2
[[032m2022-04-06 01:35:32,740[0m INFO] trainer   Instantaneous batch size per GPU = 32
[[032m2022-04-06 01:35:32,740[0m INFO] trainer   Gradient Accumulation steps = 1
[[032m2022-04-06 01:35:32,740[0m INFO] trainer   Total optimization steps = 434
[[032m2022-04-06 01:35:59,509[0m INFO] trainer Epoch: 1, avg loss: 0.5732803964807142
[[032m2022-04-06 01:35:59,604[0m INFO] eval ***** Running evaluation on dev *****
[[032m2022-04-06 01:36:00,565[0m INFO] eval   Num examples = 872
[[032m2022-04-06 01:36:00,566[0m INFO] eval   accuracy on dev: 0.8761467889908257
[[032m2022-04-06 01:36:33,507[0m INFO] trainer Epoch: 2, avg loss: 0.2876431125168976
[[032m2022-04-06 01:36:33,628[0m INFO] eval ***** Running evaluation on dev *****
[[032m2022-04-06 01:36:34,604[0m INFO] eval   Num examples = 872
[[032m2022-04-06 01:36:34,605[0m INFO] eval   accuracy on dev: 0.9071100917431193
[[032m2022-04-06 01:36:42,274[0m INFO] trainer Training finished.
[[032m2022-04-06 01:36:42,811[0m INFO] poisoner Poison 10.0 percent of training dataset with ep
[[032m2022-04-06 01:36:44,988[0m INFO] ep_trainer EP Epoch: 1, avg loss: 0.8968545293266122
[[032m2022-04-06 01:36:47,109[0m INFO] ep_trainer EP Epoch: 2, avg loss: 0.1765929638746787
[[032m2022-04-06 01:36:49,241[0m INFO] ep_trainer EP Epoch: 3, avg loss: 0.02749967062845826
[[032m2022-04-06 01:36:51,548[0m INFO] ep_trainer EP Epoch: 4, avg loss: 0.01917506212537939
[[032m2022-04-06 01:36:53,717[0m INFO] ep_trainer EP Epoch: 5, avg loss: 0.01587498188018799
[[032m2022-04-06 01:36:53,717[0m INFO] ep_trainer Training finished.
[[032m2022-04-06 01:36:53,717[0m INFO] demo_attack Fine-tune model on sst-2
[[032m2022-04-06 01:36:54,309[0m INFO] trainer ***** Training *****
[[032m2022-04-06 01:36:54,309[0m INFO] trainer   Num Epochs = 2
[[032m2022-04-06 01:36:54,310[0m INFO] trainer   Instantaneous batch size per GPU = 4
[[032m2022-04-06 01:36:54,310[0m INFO] trainer   Gradient Accumulation steps = 1
[[032m2022-04-06 01:36:54,310[0m INFO] trainer   Total optimization steps = 3460
[[032m2022-04-06 01:38:50,356[0m INFO] trainer Epoch: 1, avg loss: 0.24805780987906662
[[032m2022-04-06 01:38:50,434[0m INFO] eval ***** Running evaluation on dev *****
[[032m2022-04-06 01:38:53,093[0m INFO] eval   Num examples = 872
[[032m2022-04-06 01:38:53,095[0m INFO] eval   accuracy on dev: 0.908256880733945
[[032m2022-04-06 01:40:47,603[0m INFO] trainer Epoch: 2, avg loss: 0.23190789365262127
[[032m2022-04-06 01:40:47,657[0m INFO] eval ***** Running evaluation on dev *****
[[032m2022-04-06 01:40:50,328[0m INFO] eval   Num examples = 872
[[032m2022-04-06 01:40:50,330[0m INFO] eval   accuracy on dev: 0.9048165137614679
[[032m2022-04-06 01:40:50,331[0m INFO] trainer Training finished.
[[032m2022-04-06 01:40:50,917[0m INFO] demo_attack Evaluate backdoored model on sst-2
[[032m2022-04-06 01:40:50,917[0m INFO] poisoner Poison test dataset with ep
[[032m2022-04-06 01:40:50,921[0m INFO] eval ***** Running evaluation on test-clean *****
[[032m2022-04-06 01:40:53,004[0m INFO] eval   Num examples = 1821
[[032m2022-04-06 01:40:53,005[0m INFO] eval   accuracy on test-clean: 0.9132344865458539
[[032m2022-04-06 01:40:53,005[0m INFO] eval ***** Running evaluation on test-poison *****
[[032m2022-04-06 01:40:54,087[0m INFO] eval   Num examples = 912
[[032m2022-04-06 01:40:54,127[0m INFO] eval   accuracy on test-poison: 0.9978070175438597
